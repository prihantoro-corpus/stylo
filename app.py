import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
import networkx as nx
from scipy.spatial.distance import pdist, squareform, cdist
from scipy.cluster.hierarchy import linkage, dendrogram
from sklearn.decomposition import PCA
from sklearn.svm import SVC

# --- 1. DATA LOADING & PARSING ---
@st.cache_data(show_spinner="Fetching Corpora...")
def load_github_folder(folder):
    api_url = f"https://api.github.com/repos/prihantoro-corpus/stylo/contents/{folder}"
    raw_base_url = f"https://raw.githubusercontent.com/prihantoro-corpus/stylo/main/{folder}/"
    loaded_corpus = {}
    try:
        response = requests.get(api_url)
        if response.status_code == 200:
            files = [f['name'] for f in response.json() if f['name'].endswith(('.txt', '.tsv'))]
            for name in files:
                r = requests.get(raw_base_url + name)
                if r.status_code == 200:
                    lines = r.text.strip().split('\n')
                    if folder.endswith('preloaded2') or name.endswith('.tsv'):
                        data = [line.split('\t') for line in lines if '\t' in line]
                        loaded_corpus[name] = {
                            'word': [row[0].lower() for row in data],
                            'tag': [row[1] for row in data],
                            'lemma': [row[2].lower() for row in data]
                        }
                    else:
                        words = [w for w in r.text.lower().split() if w.isalpha()]
                        loaded_corpus[name] = {'word': words, 'tag': [], 'lemma': []}
    except: pass
    return loaded_corpus

def build_z_matrix(corpus_dict, layer, mfw_limit, stop_list=[]):
    all_tokens = []
    for doc in corpus_dict.values():
        all_tokens.extend([t for t in doc[layer] if t not in stop_list])
    top_feats = pd.Series(all_tokens).value_counts().head(mfw_limit).index
    matrix = []
    for doc in corpus_dict.values():
        counts = pd.Series(doc[layer]).value_counts()
        matrix.append(counts.reindex(top_feats, fill_value=0))
    df = pd.DataFrame(matrix, index=corpus_dict.keys())
    return (df - df.mean()) / df.std().replace(0, 1), top_feats

# --- 2. UI SETUP ---
st.set_page_config(page_title="Stylo-Lab Pro", layout="wide")
st.title("üî¨ Stylometry Lab: Attribution Edition")

with st.sidebar:
    st.header("Data Selection")
    data_source = st.radio("Corpus", ["Upload Files", "UNRESTRICTED-10", "TAGGED-10", "KNOWN-10"])
    mfw_limit = st.slider("MFW Limit", 50, 1000, 500)
    use_stop = st.checkbox("Filter Stopwords")
    stop_input = st.text_area("Stopwords", "the, and, of, to, a, in, is")
    stop_list = [w.strip().lower() for w in stop_input.split(",") if w.strip()]

# --- 3. DATA INGESTION ---
raw_data = {}
if data_source == "UNRESTRICTED-10":
    raw_data = load_github_folder("preloaded1")
elif data_source == "TAGGED-10":
    raw_data = load_github_folder("preloaded2")
elif data_source == "KNOWN-10":
    known = load_github_folder("preloaded3/known3")
    questioned = load_github_folder("preloaded3/question3")
    raw_data = {**known, **questioned}
else:
    files = st.file_uploader("Upload Files", accept_multiple_files=True)
    for f in files: # Logic for manual upload similar to previous steps...
        pass

if len(raw_data) > 2:
    # Build core Word matrix
    z_word, feats_word = build_z_matrix(raw_data, 'word', mfw_limit, stop_list if use_stop else [])

    # SCENARIO 1 & 2 remain as per previous version...
    # (Abbreviated here to focus on Scenario 3)

    if data_source == "KNOWN-10":
        st.divider()
        st.header("üîç Scenario 3: Lexical Attribution")
        
        # Split Z-matrix into Known (K) and Questioned (Q)
        k_indices = [idx for idx in z_word.index if idx.startswith('K-')]
        q_indices = [idx for idx in z_word.index if idx.startswith('Q-')]
        
        if k_indices and q_indices:
            tabs3 = st.tabs(["üó∫Ô∏è Attribution Zones", "üéØ Accuracy & Confusion", "üèÜ Delta Rank", "üï∏Ô∏è Attribution Network"])
            
            with tabs3[0]:
                st.subheader("PCA Map with Authorship Zones")
                # Assign labels for training zones (assuming format K-AuthorName-File.txt)
                labels = [n.split('-')[1] if '-' in n else 'Unknown' for n in k_indices]
                pca_model = PCA(n_components=2)
                coords = pca_model.fit_transform(z_word)
                
                fig, ax = plt.subplots(figsize=(10, 6))
                # Create background zones using SVM
                if len(set(labels)) > 1:
                    svc = SVC(kernel='linear').fit(coords[:len(k_indices)], labels)
                    x_min, x_max = coords[:, 0].min() - 1, coords[:, 0].max() + 1
                    y_min, y_max = coords[:, 1].min() - 1, coords[:, 1].max() + 1
                    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
                    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
                    ax.contourf(xx, yy, Z.reshape(xx.shape), alpha=0.2, cmap='Set3')
                
                ax.scatter(coords[:len(k_indices),0], coords[:len(k_indices),1], c='blue', label='Known')
                ax.scatter(coords[len(k_indices):,0], coords[len(k_indices):,1], c='red', marker='x', label='Questioned')
                for i, txt in enumerate(z_word.index): ax.annotate(txt, (coords[i,0], coords[i,1]), size=8)
                st.pyplot(fig)
                

            with tabs3[1]:
                st.subheader("Confusion Matrix & Accuracy")
                # Simple Delta calculation for Attribution
                dist_mat = cdist(z_word.loc[q_indices], z_word.loc[k_indices], metric='cityblock')
                predictions = [labels[np.argmin(row)] for row in dist_mat]
                
                # Mock Accuracy based on filename prefixes if possible
                actual = [n.split('-')[1] for n in q_indices if '-' in n]
                if actual:
                    acc = sum([1 for p, a in zip(predictions, actual) if p == a]) / len(actual)
                    st.metric("Overall Attribution Accuracy", f"{acc:.2%}")
                
                conf_df = pd.DataFrame(dist_mat, index=q_indices, columns=k_indices)
                st.write("Distance Matrix (Lower = Higher Probability)")
                st.dataframe(conf_df.style.background_gradient(cmap='YlGn_r'))

            with tabs3[2]:
                st.subheader("Delta Rank Table")
                results = []
                for i, q in enumerate(q_indices):
                    best_match_idx = np.argmin(dist_mat[i])
                    results.append({"Questioned": q, "Top Candidate": k_indices[best_match_idx], "Distance": dist_mat[i][best_match_idx]})
                
                res_df = pd.DataFrame(results)
                st.table(res_df)

                # NARRATION
                st.info("### üìù Attribution Summary")
                for _, row in res_df.iterrows():
                    st.write(f"The text **{row['Questioned']}** is likely to be written by the same person who wrote **{row['Top Candidate']}**.")

else:
    st.info("Load KNOWN-10 to enable Scenario 3 Attribution.")
