import re
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
import networkx as nx
from scipy.spatial.distance import pdist, squareform, cdist
from scipy.cluster.hierarchy import linkage, dendrogram
from sklearn.decomposition import PCA
from sklearn.svm import SVC


# --- 1. DATA LOADING & MULTI-LAYER PARSING ---
@st.cache_data(show_spinner="Fetching Corpora...")
def load_corpus(folder_path):
Â  Â  """
Â  Â  Recursively fetches files from GitHub to handle nested subfolders
Â  Â  like preloaded3/known3 and preloaded3/question3.
Â  Â  """
Â  Â  api_base = "https://api.github.com/repos/prihantoro-corpus/stylo/contents"
Â  Â  raw_base = "https://raw.githubusercontent.com/prihantoro-corpus/stylo/main"
Â  Â  corpus = {}

Â  Â  def fetch_recursive(current_path):
Â  Â  Â  Â  api_url = f"{api_base}/{current_path}"
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  response = requests.get(api_url)
Â  Â  Â  Â  Â  Â  if response.status_code == 200:
Â  Â  Â  Â  Â  Â  Â  Â  items = response.json()
Â  Â  Â  Â  Â  Â  Â  Â  for item in items:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if item['type'] == 'dir':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Dig into subdirectories
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fetch_recursive(f"{current_path}/{item['name']}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif item['name'].endswith(('.txt', '.tsv')):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raw_url = f"{raw_base}/{current_path}/{item['name']}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r = requests.get(raw_url)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if r.status_code == 200:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lines = r.text.strip().split('\n')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Detect TreeTagger (3 columns)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if '\t' in lines[0]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data = [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  line.split('\t') for line in lines
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if '\t' in line
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  corpus[item['name']] = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'word': [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  row[0].lower() for row in data
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(row) > 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'tag':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  [row[1] for row in data if len(row) > 1],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lemma': [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  row[2].lower() for row in data
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(row) > 2
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:Â  # Plain Text
# This regex splits by whitespace but keeps punctuation as separate tokens
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  words = re.findall(r"[\w']+|[.,!?;:()\"-]", r.text.lower())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  corpus[item['name']] = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'word': words,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'tag': [],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lemma': []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  except:
Â  Â  Â  Â  Â  Â  pass

Â  Â  fetch_recursive(folder_path)
Â  Â  return corpus


def build_matrix(corpus_dict, layer, mfw_limit, n_size=1, stops=[]):
Â  Â  all_ngram_tokens = []
Â  Â Â 
Â  Â  # helper to create n-grams from a list of tokens
Â  Â  def get_ngrams(tokens, n):
Â  Â  Â  Â  if n == 1:
Â  Â  Â  Â  Â  Â  return tokens
Â  Â  Â  Â  return [" ".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

Â  Â  for doc in corpus_dict.values():
Â  Â  Â  Â  tokens = [t for t in doc[layer] if t not in stops]
Â  Â  Â  Â  ngrams = get_ngrams(tokens, n_size)
Â  Â  Â  Â  all_ngram_tokens.extend(ngrams)

Â  Â  if not all_ngram_tokens:
Â  Â  Â  Â  return pd.DataFrame(), []

Â  Â  top_feats = pd.Series(all_ngram_tokens).value_counts().head(mfw_limit).index

Â  Â  matrix = []
Â  Â  for doc in corpus_dict.values():
Â  Â  Â  Â  doc_tokens = [t for t in doc[layer] if t not in stops]
Â  Â  Â  Â  doc_ngrams = get_ngrams(doc_tokens, n_size)
Â  Â  Â  Â  counts = pd.Series(doc_ngrams).value_counts()
Â  Â  Â  Â  matrix.append(counts.reindex(top_feats, fill_value=0))

Â  Â  df = pd.DataFrame(matrix, index=corpus_dict.keys())
Â  Â  df_std = df.std().replace(0, 1)
Â  Â  z_scores = (df - df.mean()) / df_std
Â  Â  return z_scores.fillna(0), top_feats

# --- 2. APP CONFIG & SIDEBAR ---
st.set_page_config(page_title="Stylo-Lab Professional", layout="wide")
st.title("ğŸ”¬ Stylometry Lab: Lexical, Structural & Attribution")

with st.sidebar:
Â  Â  st.header("Selection")
Â  Â  data_source = st.radio("Corpus", ["UNRESTRICTED-10", "TAGGED-10", "KNOWN-10", "TAGGED-ATTRIBUTION", "Upload Files"])
Â  Â  mfw_limit = st.slider("MFW Limit", 50, 2000, 500)
Â  Â  use_stop = st.checkbox("Filter Stopwords", value=True)
Â  Â  stop_input = st.text_area(
Â  Â  Â  Â  "Stopwords", "the, and, of, to, a, in, is, it, that, was").lower()
Â  Â  stop_list = [w.strip() for w in stop_input.split(",") if w.strip()]

Â  Â  st.markdown("---")
Â  Â  st.header("Network Settings")
Â  Â  net_threshold = st.slider("Connection Sensitivity (Percentile)", 5, 95, 25)
Â  Â  n_size = st.slider("N-Gram Size (Phrasal patterns)", 1, 5, 1)
Â  Â  st.caption("1 = Single Word, 2 = Bigram (2 words), etc. Higher values capture specific phrasing.")

# --- 3. DATA PROCESSING ---
raw_data = {}
if data_source == "UNRESTRICTED-10":
Â  Â  raw_data = load_corpus("preloaded1")
elif data_source == "TAGGED-10":
Â  Â  raw_data = load_corpus("preloaded2")
elif data_source == "KNOWN-10":
Â  Â  raw_data = load_corpus("preloaded3")
elif data_source == "TAGGED-ATTRIBUTION":
Â  Â  raw_data = load_corpus("preloaded4")
else:
Â  Â  uploaded = st.file_uploader("Upload .txt or .tsv files",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  accept_multiple_files=True)
Â  Â  for f in uploaded:
Â  Â  Â  Â  content = f.read().decode("utf-8")
Â  Â  Â  Â  if f.name.endswith('.tsv'):
Â  Â  Â  Â  Â  Â  data = [
Â  Â  Â  Â  Â  Â  Â  Â  line.split('\t') for line in content.strip().split('\n')
Â  Â  Â  Â  Â  Â  Â  Â  if '\t' in line
Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  raw_data[f.name] = {
Â  Â  Â  Â  Â  Â  Â  Â  'word': [r[0].lower() for r in data],
Â  Â  Â  Â  Â  Â  Â  Â  'tag': [r[1] for r in data],
Â  Â  Â  Â  Â  Â  Â  Â  'lemma': [r[2].lower() for r in data]
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  raw_data[f.name] = {
Â  Â  Â  Â  Â  Â  Â  Â  'word': [w for w in content.lower().split() if w.isalpha()],
Â  Â  Â  Â  Â  Â  Â  Â  'tag': [],
Â  Â  Â  Â  Â  Â  Â  Â  'lemma': []
Â  Â  Â  Â  Â  Â  }

# --- 4. ANALYTICS ENGINES ---
if len(raw_data) >= 2:

Â  Â  z_word, feats_word = build_matrix(raw_data, 'word', mfw_limit, n_size=n_size, stops=stop_list if use_stop else [])

Â  Â  # --- SCENARIO 1: LEXICAL EXPLORER ---
Â  Â  st.header("ğŸ“¦ Scenario 1: Lexical Explorer (Words)")
Â  Â  t1, t2, t3, t4, t5 = st.tabs([
Â  Â  Â  Â  "ğŸŒ³ Dendrogram", "ğŸ—ºï¸ PCA Map", "ğŸ“ˆ Loadings", "ğŸ•¸ï¸ Network", "ğŸ“Š CSV Data"
Â  Â  ])

Â  Â  with t1:
Â  Â  Â  Â  st.subheader("Hierarchical Clustering")
Â  Â  Â  Â  fig, ax = plt.subplots(figsize=(10, 5))
Â  Â  Â  Â  dendrogram(linkage(z_word, 'ward'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â labels=list(raw_data.keys()),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â orientation='left',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ax=ax)
Â  Â  Â  Â  st.pyplot(fig)

Â  Â  Â  Â  # New suggestion logic
Â  Â  Â  Â  st.info("ğŸ’¡ **Cluster Suggestion**")
Â  Â  Â  Â  last = linkage(z_word, 'ward')[-10:, 2]
Â  Â  Â  Â  acceleration = np.diff(last, 2)
Â  Â  Â  Â  suggested_k = acceleration.argmax() + 2
Â  Â  Â  Â  st.write(
Â  Â  Â  Â  Â  Â  f"Based on variance, **{suggested_k} clusters** is likely optimal."
Â  Â  Â  Â  )

Â  Â  with t2:
Â  Â  Â  Â  st.subheader("PCA Visualization")
Â  Â  Â  Â  pca_coords = PCA(n_components=2).fit_transform(z_word)
Â  Â  Â  Â  fig, ax = plt.subplots()
Â  Â  Â  Â  ax.scatter(pca_coords[:, 0],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â pca_coords[:, 1],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â c='skyblue',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â edgecolors='navy')
Â  Â  Â  Â  for i, txt in enumerate(raw_data.keys()):
Â  Â  Â  Â  Â  Â  ax.annotate(txt, (pca_coords[i, 0], pca_coords[i, 1]), size=8)
Â  Â  Â  Â  st.pyplot(fig)

Â  Â  with t3:
Â  Â  Â  Â  pca_model = PCA(n_components=2).fit(z_word)
Â  Â  Â  Â  loadings = pd.DataFrame(pca_model.components_.T,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  index=feats_word,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  columns=['PC1', 'PC2'])
Â  Â  Â  Â  st.write("Top Words driving differences (PC1):")
Â  Â  Â  Â  st.dataframe(loadings.sort_values('PC1', ascending=False).head(20))

#--------------nih
Â  Â  with t4:
Â  Â  Â  Â  st.subheader("Document Network")
Â  Â  Â  Â  G = nx.Graph()
Â  Â  Â  Â  dist_matrix = squareform(pdist(z_word, metric='cityblock'))
Â  Â  Â  Â  # Use the slider value from the sidebar here:
Â  Â  Â  Â  threshold = np.percentile(dist_matrix, net_threshold)
Â  Â  Â  Â  for i, ni in enumerate(raw_data.keys()):
Â  Â  Â  Â  Â  Â  #--------------------------- nih
Â  Â  Â  Â  Â  Â  for j, nj in enumerate(raw_data.keys()):
Â  Â  Â  Â  Â  Â  Â  Â  if i < j and dist_matrix[i, j] < threshold: G.add_edge(ni, nj)
Â  Â  Â  Â  fig, ax = plt.subplots()
Â  Â  Â  Â  nx.draw(G, with_labels=True, node_color='orange', ax=ax, font_size=8)
Â  Â  Â  Â  st.pyplot(fig)

Â  Â  with t5:
Â  Â  Â  Â  st.download_button("Download Z-Scores (CSV)", z_word.to_csv(),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â "lexical_zscores.csv")

Â  Â  Â  Â  search_query = st.text_input(
Â  Â  Â  Â  Â  Â  "ğŸ” Search for a specific word in the Z-scores:")
Â  Â  Â  Â  if search_query:
Â  Â  Â  Â  Â  Â  # Filters columns that contain the search string
Â  Â  Â  Â  Â  Â  filtered_df = z_word[z_word.columns[z_word.columns.str.contains(
Â  Â  Â  Â  Â  Â  Â  Â  search_query.lower())]]
Â  Â  Â  Â  Â  Â  st.dataframe(filtered_df)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.dataframe(z_word)

Â  Â  # --- SCENARIO 2: STRUCTURAL EXPLORER (Only if Tags Exist) ---
Â  Â  if any(raw_data[next(iter(raw_data))]['tag']):
Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  st.header("ğŸ§¬ Scenario 2: Structural Explorer (Tags & Lemmas)")

Â  Â  Â  Â  z_tag, feats_tag = build_matrix(raw_data, 'tag', 100)
Â  Â  Â  Â  z_lemma, feats_lemma = build_matrix(raw_data, 'lemma', mfw_limit)

Â  Â  Â  Â  st2_t1, st2_t2, st2_t3, st2_t4 = st.tabs([
Â  Â  Â  Â  Â  Â  "ğŸŒ³ Dual Dendrograms", "ğŸ—ºï¸ Grammar Map", "ğŸ“ˆ Tag Loadings",
Â  Â  Â  Â  Â  Â  "ğŸ“Š Grammar Profile"
Â  Â  Â  Â  ])
Â  Â  Â  Â  with st2_t1:
Â  Â  Â  Â  Â  Â  col1, col2 = st.columns(2)
Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  st.write("Lemma-based Clustering")
Â  Â  Â  Â  Â  Â  Â  Â  f1, a1 = plt.subplots()
Â  Â  Â  Â  Â  Â  Â  Â  dendrogram(linkage(z_lemma, 'ward'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â labels=list(raw_data.keys()),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â orientation='left',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ax=a1)
Â  Â  Â  Â  Â  Â  Â  Â  st.pyplot(f1)
Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  st.write("Tag-based (Grammar) Clustering")
Â  Â  Â  Â  Â  Â  Â  Â  f2, a2 = plt.subplots()
Â  Â  Â  Â  Â  Â  Â  Â  dendrogram(linkage(z_tag, 'ward'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â labels=list(raw_data.keys()),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â orientation='left',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ax=a2)
Â  Â  Â  Â  Â  Â  Â  Â  st.pyplot(f2)

Â  Â  Â  Â  with st2_t2:
Â  Â  Â  Â  Â  Â  pca_tag = PCA(n_components=2).fit_transform(z_tag)
Â  Â  Â  Â  Â  Â  fig, ax = plt.subplots()
Â  Â  Â  Â  Â  Â  ax.scatter(pca_tag[:, 0], pca_tag[:, 1], c='green')
Â  Â  Â  Â  Â  Â  for i, txt in enumerate(raw_data.keys()):
Â  Â  Â  Â  Â  Â  Â  Â  ax.annotate(txt, (pca_tag[i, 0], pca_tag[i, 1]))
Â  Â  Â  Â  Â  Â  st.pyplot(fig)

Â  Â  Â  Â  with st2_t3:
Â  Â  Â  Â  Â  Â  tag_loads = pd.DataFrame(
Â  Â  Â  Â  Â  Â  Â  Â  PCA(n_components=2).fit(z_tag).components_.T,
Â  Â  Â  Â  Â  Â  Â  Â  index=feats_tag,
Â  Â  Â  Â  Â  Â  Â  Â  columns=['PC1', 'PC2'])
Â  Â  Â  Â  Â  Â  st.dataframe(tag_loads.sort_values('PC1', ascending=False))

Â  Â  Â  Â  with st2_t4:
Â  Â  Â  Â  Â  Â  st.subheader("POS Ratios")
Â  Â  Â  Â  Â  Â  profile_data = []
Â  Â  Â  Â  Â  Â  for name, d in raw_data.items():
Â  Â  Â  Â  Â  Â  Â  Â  ratios = pd.Series(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  d['tag']).value_counts(normalize=True).head(10)
Â  Â  Â  Â  Â  Â  Â  Â  profile_data.append(ratios)
Â  Â  Â  Â  Â  Â  profile_df = pd.DataFrame(profile_data,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  index=raw_data.keys()).fillna(0)
Â  Â  Â  Â  Â  Â  st.bar_chart(profile_df)

# --- SCENARIO 3: ATTRIBUTION ---
Â  Â  # We allow this for KNOWN-10 OR Uploaded files so long as K/Q naming exists
Â  Â  if data_source in ["KNOWN-10", "TAGGED-ATTRIBUTION", "Upload Files"]:
Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  st.header("ğŸ” Scenario 3: Lexical Attribution")

Â  Â  Â  Â  k_idx = [i for i in z_word.index if i.startswith('K-')]
Â  Â  Â  Â  q_idx = [i for i in z_word.index if i.startswith('Q-')]

Â  Â  Â  Â  with st.expander("ğŸ“‚ View Loaded Data Inventory"):
Â  Â  Â  Â  Â  Â  st.write(f"Known Files: {len(k_idx)} found.")
Â  Â  Â  Â  Â  Â  st.write(f"Questioned Files: {len(q_idx)} found.")

Â  Â  Â  Â  if len(k_idx) >= 2 and len(q_idx) >= 1:
Â  Â  Â  Â  Â  Â  # 1. Global Calculations for all tabs
Â  Â  Â  Â  Â  Â  labels = [n.split('-')[1] if '-' in n else "Unknown" for n in k_idx]
Â  Â  Â  Â  Â  Â  pca_mod = PCA(n_components=2).fit(z_word)
Â  Â  Â  Â  Â  Â  coords = pca_mod.transform(z_word)

Â  Â  Â  Â  Â  Â  at1, at2, at3, at4 = st.tabs([
Â  Â  Â  Â  Â  Â  Â  Â  "ğŸ—ºï¸ Attribution Zones", "ğŸ¯ Accuracy/Confusion", "ğŸ† Delta Rank", "ğŸ”‘ Known Markers"
Â  Â  Â  Â  Â  Â  ])

Â  Â  Â  Â  Â  Â  with at1:
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Authorship Zones (SVM)")
Â  Â  Â  Â  Â  Â  Â  Â  fig, ax = plt.subplots(figsize=(10, 7))
Â  Â  Â  Â  Â  Â  Â  Â  if len(set(labels)) > 1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  svc = SVC(kernel='linear').fit(coords[:len(k_idx)], labels)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  x_min, x_max = coords[:, 0].min() - 1, coords[:, 0].max() + 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  y_min, y_max = coords[:, 1].min() - 1, coords[:, 1].max() + 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label_map = {name: i for i, name in enumerate(sorted(list(set(labels))))}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Z_num = np.array([label_map[z] for z in Z]).reshape(xx.shape)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.contourf(xx, yy, Z_num, alpha=0.15, cmap='coolwarm')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except: pass
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  ax.scatter(coords[:len(k_idx), 0], coords[:len(k_idx), 1], c='blue', label='Known')
Â  Â  Â  Â  Â  Â  Â  Â  ax.scatter(coords[len(k_idx):, 0], coords[len(k_idx):, 1], c='red', marker='X', s=100, label='Questioned')
Â  Â  Â  Â  Â  Â  Â  Â  for i, txt in enumerate(z_word.index):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.annotate(txt, (coords[i, 0], coords[i, 1]), size=8)
Â  Â  Â  Â  Â  Â  Â  Â  ax.legend()
Â  Â  Â  Â  Â  Â  Â  Â  st.pyplot(fig)

Â  Â  Â  Â  Â  Â  with at2:
Â  Â  Â  Â  Â  Â  Â  Â  dist_mat = cdist(z_word.loc[q_idx], z_word.loc[k_idx], metric='cityblock')
Â  Â  Â  Â  Â  Â  Â  Â  st.write("### Distance Matrix (Manhattan Distance)")
Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(pd.DataFrame(dist_mat, index=q_idx, columns=k_idx).style.background_gradient(cmap='RdYlGn_r'))

Â  Â  Â  Â  Â  Â  with at3:
Â  Â  Â  Â  Â  Â  Â  Â  results = []
Â  Â  Â  Â  Â  Â  Â  Â  dist_mat = cdist(z_word.loc[q_idx], z_word.loc[k_idx], metric='cityblock')
Â  Â  Â  Â  Â  Â  Â  Â  k_internal_dists = pdist(z_word.loc[k_idx], metric='cityblock')
Â  Â  Â  Â  Â  Â  Â  Â  outlier_threshold = np.mean(k_internal_dists) + np.std(k_internal_dists)

Â  Â  Â  Â  Â  Â  Â  Â  for i, q in enumerate(q_idx):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dists = dist_mat[i]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  min_dist = np.min(dists)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sorted_indices = np.argsort(dists)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  match_idx, runner_up_idx = sorted_indices[0], sorted_indices[1]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  confidence = (dists[runner_up_idx] - dists[match_idx]) / dists[runner_up_idx]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_outlier = min_dist > outlier_threshold
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Questioned": q, "Top Match": k_idx[match_idx],Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Confidence": f"{confidence:.2%}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Status": "Outlier" if is_outlier else "Closely Similar",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Dist_Val": min_dist
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â  Â  Â  def color_status(val):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  color = 'red' if val == 'Outlier' else 'green'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return f'color: {color}; font-weight: bold'

Â  Â  Â  Â  Â  Â  Â  Â  df_display = pd.DataFrame(results).drop(columns=['Dist_Val'])
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("ğŸ† Attribution & Delta Rank")
Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(df_display.style.map(color_status, subset=['Status']), use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  st.download_button("ğŸ“¥ Download Results (CSV)", df_display.to_csv(index=False), "attribution.csv")

Â  Â  Â  Â  Â  Â  Â  Â  st.info("### ğŸ“ Automated Authorship Narration")
Â  Â  Â  Â  Â  Â  Â  Â  similar_texts = [r['Questioned'] for r in results if r['Status'] == "Closely Similar"]
Â  Â  Â  Â  Â  Â  Â  Â  outlier_texts = [r['Questioned'] for r in results if r['Status'] == "Outlier"]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if similar_texts:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**âœ… Possibility One (High Similarity):**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"The texts **{', '.join(similar_texts)}** fall within expected variance.")
Â  Â  Â  Â  Â  Â  Â  Â  if outlier_texts:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**âš ï¸ Possibility Two (Stylistic Outliers):**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"The texts **{', '.join(outlier_texts)}** are statistically distant.")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("ğŸ Final Verdict Summary")
Â  Â  Â  Â  Â  Â  Â  Â  cv1, cv2, cv3 = st.columns(3)
Â  Â  Â  Â  Â  Â  Â  Â  cv1.metric("Total", len(results))
Â  Â  Â  Â  Â  Â  Â  Â  cv2.metric("Attributed", len(similar_texts))
Â  Â  Â  Â  Â  Â  Â  Â  cv3.metric("Outliers", len(outlier_texts))

Â  Â  Â  Â  Â  Â  with at4:
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("ğŸ”‘ Key Stylistic Markers of Known Texts")
Â  Â  Â  Â  Â  Â  Â  Â  known_mean_coord = np.mean(coords[:len(k_idx), 0])
Â  Â  Â  Â  Â  Â  Â  Â  direction = 1 if known_mean_coord > 0 else -1
Â  Â  Â  Â  Â  Â  Â  Â  weights = pca_mod.components_[0] * direction
Â  Â  Â  Â  Â  Â  Â  Â  marker_df = pd.DataFrame({'Word': list(feats_word), 'Weight': weights}).sort_values('Weight', ascending=False)
Â  Â  Â  Â  Â  Â  Â  Â  top_markers = marker_df[marker_df['Weight'] > 0].head(15)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if not top_markers.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  col_m1, col_m2 = st.columns([1, 2])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col_m1: st.dataframe(top_markers, hide_index=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col_m2: st.bar_chart(top_markers.set_index('Word'))

Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.warning("Insufficient data. Ensure filenames start with 'K-' and 'Q-'.")
#=======
# --- SCENARIO 4: GRAMMATICAL PROFILER (New Feature) ---
Â  Â  # Only trigger if the files have TAGS (TreeTagger format)
Â  Â  if any(raw_data[next(iter(raw_data))]['tag']):
Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  st.header("ğŸ§¬ Scenario 4: Grammatical Profiler (POS Markers)")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Build matrix using 'tag' layer
Â  Â  Â  Â  z_tag_attr, feats_tag_attr = build_matrix(raw_data, 'tag', 100)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Use the same K- and Q- indices from Scenario 3
Â  Â  Â  Â  k_idx = [i for i in z_word.index if i.startswith('K-')]
Â  Â  Â  Â Â 
Â  Â  Â  Â  if len(k_idx) >= 2:
Â  Â  Â  Â  Â  Â  pca_tag_attr = PCA(n_components=2).fit(z_tag_attr)
Â  Â  Â  Â  Â  Â  tag_coords = pca_tag_attr.transform(z_tag_attr)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Determine which direction the 'Known' group leans grammatically
Â  Â  Â  Â  Â  Â  k_dir_tag = 1 if np.mean(tag_coords[:len(k_idx), 0]) > 0 else -1
Â  Â  Â  Â  Â  Â  tag_weights = pca_tag_attr.components_[0] * k_dir_tag
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  marker_tag_df = pd.DataFrame({
Â  Â  Â  Â  Â  Â  Â  Â  'POS Tag': feats_tag_attr,Â 
Â  Â  Â  Â  Â  Â  Â  Â  'Strength': tag_weights
Â  Â  Â  Â  Â  Â  }).sort_values('Strength', ascending=False)

Â  Â  Â  Â  Â  Â  st.write("### ğŸ”‘ Key Grammatical Markers of Known Author")
Â  Â  Â  Â  Â  Â  st.info("These POS (Part-of-Speech) tags represent the structural 'fingerprint' of the Known author.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  col_g1, col_g2 = st.columns([1, 2])
Â  Â  Â  Â  Â  Â  with col_g1:
Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(marker_tag_df.head(15), hide_index=True)
Â  Â  Â  Â  Â  Â  with col_g2:
Â  Â  Â  Â  Â  Â  Â  Â  # Visualize the top 10 markers
Â  Â  Â  Â  Â  Â  Â  Â  st.bar_chart(marker_tag_df.head(10).set_index('POS Tag'))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.caption("Common Tags: NN (Noun), VBD (Verb Past), MD (Modal), JJ (Adjective).")
#=======
else:
Â  Â  st.info("Please load or upload at least 2 files to generate analytics.")
